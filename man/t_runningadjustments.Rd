% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{t_running_centered}
\alias{t_running_centered}
\alias{t_running_scaled}
\alias{t_running_zscored}
\alias{t_running_sharpe}
\alias{t_running_tstat}
\title{Compare data to moments computed over a time sliding window.}
\usage{
t_running_centered(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, na_rm = FALSE, min_df = 0L, used_df = 1, lookahead = 0,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_scaled(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, na_rm = FALSE, min_df = 0L, used_df = 1, lookahead = 0,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_zscored(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, na_rm = FALSE, min_df = 0L, used_df = 1, lookahead = 0,
  restart_period = 100L, variable_win = FALSE, wts_as_delta = TRUE,
  check_wts = FALSE, normalize_wts = TRUE)

t_running_sharpe(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, compute_se = FALSE,
  min_df = 0L, used_df = 1, restart_period = 100L, variable_win = FALSE,
  wts_as_delta = TRUE, check_wts = FALSE, normalize_wts = TRUE)

t_running_tstat(v, time = NULL, time_deltas = NULL, window = NULL,
  wts = NULL, lb_time = NULL, na_rm = FALSE, compute_se = FALSE,
  min_df = 0L, used_df = 1, restart_period = 100L, variable_win = FALSE,
  wts_as_delta = TRUE, check_wts = FALSE, normalize_wts = TRUE)
}
\arguments{
\item{v}{a vector of data.}

\item{time}{an optional vector of the timestamps of \code{v}. If given, must be
the same length as \code{v}. If not given, we try to infer it by summing the
\code{time_deltas}.}

\item{time_deltas}{an optional vector of the deltas of timestamps. If given, must be
the same length as \code{v}. If not given, and \code{wts} are given and \code{wts_as_delta} is true,
we take the \code{wts} as the time deltas.  The deltas must be positive. We sum them to arrive
at the times.}

\item{window}{the window size, in time units. if given as finite integer or double, passed through.
If \code{NULL}, \code{NA_integer_}, \code{NA_real_} or \code{Inf} are given, 
 and \code{variable_win} is true, then we infer the window from the lookback times: the
 first window is infinite, but the remaining is the deltas between lookback times.
 If \code{variable_win} is false, then these undefined values are equivalent to an
 infinite window.
 If negative, an error will be thrown.}

\item{wts}{an optional vector of weights. Weights are \sQuote{replication}
weights, meaning a value of 2 is shorthand for having two observations
with the corresponding \code{v} value. If \code{NULL}, corresponds to
equal weights, the default. Note that weights are typically only meaningfully defined
up to a multiplicative constant, meaning the units of weights are
immaterial. When weights are \code{NA}, the same rules for checking \code{v}
are applied.}

\item{na_rm}{whether to remove NA, false by default.}

\item{min_df}{the minimum df to return a value, otherwise \code{NaN} is returned.
This can be used to prevent \emph{e.g.} Z-scores from being computed on only 3
observations. Defaults to zero, meaning no restriction, which can result in 
infinite Z-scores during the burn-in period.}

\item{used_df}{the number of degrees of freedom consumed, used in the denominator
of the centered moments computation. These are subtracted from the number of
observations.}

\item{lookahead}{for some of the operations, the value is compared to 
mean and standard deviation possibly using 'future' or 'past' information
by means of a non-zero lookahead. Positive values mean data are taken from
the future. This is in time units, and so should be a real.}

\item{restart_period}{the recompute period. Because subtraction of elements can cause
loss of precision, the computation of moments is restarted periodically based on 
this parameter. Larger values mean fewer restarts and faster, though less accurate
results. Recomputation is sparked when a critical number of subtractions have
been reached. Note that the code checks for negative second and fourth moments and
recomputes when needed. (Really?)}

\item{variable_win}{if true, and the \code{window} is not a concrete number,
the computation window becomes the time between lookback times.}

\item{wts_as_delta}{if true and the \code{time} and \code{time_deltas} are not
given, but \code{wts} are given, we take \code{wts} as the \code{time_deltas}.}

\item{check_wts}{a boolean for whether the code shall check for negative
weights, and throw an error when they are found. Default false for speed.}

\item{normalize_wts}{a boolean for whether the weights should be
renormalized to have a mean value of 1. This mean is computed over elements
which contribute to the moments, so if \code{na_rm} is set, that means non-NA
elements of \code{wts} that correspond to non-NA elements of the data
vector.}

\item{lb_time}{a vector of the times from which lookback will be performed. The output should
be the same size as this vector. If not given, defaults to \code{time}.}

\item{compute_se}{for \code{running_sharpe}, return an extra column of the
standard error, as computed by Mertens' correction.}
}
\value{
a vector the same size as the input consisting of the adjusted version of the input.
When there are not sufficient (non-nan) elements for the computation, \code{NaN} are returned.
}
\description{
Computes moments over a sliding window, then adjusts the data accordingly, centering, or scaling,
or z-scoring, and so on.
}
\details{
Given the length \eqn{n} vector \eqn{x}, for
a given index \eqn{i}, define \eqn{x^{(i)}}{x^(i)}
as the elements of \eqn{x} defined by the sliding time window (see the section
on time windowing).
Then define \eqn{\mu_i}{mu_i}, \eqn{\sigma_i}{sigma_i}
and \eqn{n_i}{n_i} as, respectively, the sample mean, standard deviation and number of
non-NA elements in \eqn{x^{(i)}}{x^(i)}. 

We compute output vector \eqn{m} the same size as \eqn{x}. 
For the 'centered' version of \eqn{x}, we have \eqn{m_i = x_i - \mu_i}{m_i = x_i - mu_i}.
For the 'scaled' version of \eqn{x}, we have \eqn{m_i = x_i / \sigma_i}{m_i = x_i / sigma_i}.
For the 'z-scored' version of \eqn{x}, we have \eqn{m_i = (x_i - \mu_i) / \sigma_i}{m_i = (x_i - mu_i) / sigma_i}.
For the 't-scored' version of \eqn{x}, we have \eqn{m_i = \sqrt{n_i} \mu_i / \sigma_i}{m_i = sqrt(n_i) mu_i / sigma_i}.

We also allow a 'lookahead' for some of these operations.
If positive, the moments are computed using data from larger indices;
if negative, from smaller indices.
}
\note{
The moment computations provided by fromo are 
numerically robust, but will often \emph{not} provide the
same results as the 'standard' implementations,
due to differences in roundoff. We make every attempt to balance
speed and robustness. User assumes all risk from using
the fromo package.
}
\references{
Terriberry, T. "Computing Higher-Order Moments Online."
\url{http://people.xiph.org/~tterribe/notes/homs.html}

J. Bennett, et. al., "Numerically Stable, Single-Pass, 
Parallel Statistics Algorithms," Proceedings of IEEE
International Conference on Cluster Computing, 2009.
\url{https://www.semanticscholar.org/paper/Numerically-stable-single-pass-parallel-statistics-Bennett-Grout/a83ed72a5ba86622d5eb6395299b46d51c901265}

Cook, J. D. "Accurately computing running variance."
\url{http://www.johndcook.com/standard_deviation.html}

Cook, J. D. "Comparing three methods of computing 
standard deviation."
\url{http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation}
}
\seealso{
\code{\link{running_centered}}, \code{\link{scale}}
}
\author{
Steven E. Pav \email{shabbychef@gmail.com}
}
